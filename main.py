import pandas as pd
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    pipeline
)
import numpy as np
from evaluate import load as load_metric
import torch
import os
import shutil
from torch.utils.data import DataLoader

print(torch.version.cuda)  # æŸ¥çœ‹ PyTorch å…¼å®¹çš„ CUDA ç‰ˆæœ¬

# ç¡®ä¿ sklearn å·²å®‰è£…ï¼Œå¦åˆ™å®‰è£…
try:
    from sklearn.metrics import accuracy_score
except ImportError:
    os.system("pip install scikit-learn")

# ç›®å½•è·¯å¾„
output_dir = "imdb-distilbert-finetuned"
log_dir = "logs"

# **æ£€æŸ¥ logs æ˜¯å¦å·²ç»å­˜åœ¨**
if os.path.exists(log_dir):
    if not os.path.isdir(log_dir):  
        print(f"âš ï¸ Error: {log_dir} exists but is not a directory. Removing it...")
        os.remove(log_dir)  # **åˆ é™¤é”™è¯¯çš„æ–‡ä»¶**
    else:
        print(f"ğŸ—‘ Removing existing directory: {log_dir}")
        shutil.rmtree(log_dir)  # **åˆ é™¤æ•´ä¸ªç›®å½•**

# **ç¡®ä¿ logs ç›®å½•æ­£ç¡®åˆ›å»º**
try:
    os.makedirs(log_dir, exist_ok=True)
    print(f"âœ… Directory '{log_dir}' successfully created!")
except Exception as e:
    print(f"âŒ Failed to create directory '{log_dir}': {e}")
    exit(1)  # **å¼ºåˆ¶é€€å‡ºï¼Œé¿å…åç»­é”™è¯¯**

# æ‰“å° PyTorch ç‰ˆæœ¬ä¿¡æ¯
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")

# åŠ è½½ IMDb æ•°æ®é›†
raw_datasets = load_dataset("imdb")

# åŠ è½½é¢„è®­ç»ƒçš„ tokenizer
model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# å®šä¹‰ tokenization è¿‡ç¨‹
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=256)

# é¢„å¤„ç†æ•°æ®é›†
encoded_datasets = raw_datasets.map(tokenize_function, batched=True)

# æ•°æ®æ‹†åˆ†ï¼ˆè®­ç»ƒé›† 90%ï¼ŒéªŒè¯é›† 10%ï¼‰
small_train_dataset = encoded_datasets["train"].train_test_split(test_size=0.1)
train_dataset = small_train_dataset["train"]
eval_dataset = small_train_dataset["test"]
test_dataset = encoded_datasets["test"]

# **ğŸ’¡ ä¿®æ­£ train_dataloader ä½ç½®**
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)

# åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹
num_labels = 2
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir=output_dir,
    logging_dir=log_dir,  # âœ… ç¡®ä¿ logs ç›®å½•æ­£ç¡®
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=100,
    load_best_model_at_end=True
)

# è®¡ç®—å‡†ç¡®ç‡çš„å‡½æ•°
accuracy_metric = load_metric("accuracy")

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return accuracy_metric.compute(predictions=predictions, references=labels)

# åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,  # æœªæ¥å°†è¢«ç§»é™¤
    compute_metrics=compute_metrics
)

# è®­ç»ƒæ¨¡å‹
trainer.train()

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
metrics = trainer.evaluate(test_dataset)
print("Test set metrics:", metrics)

# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
fine_tuned_model_path = "fine_tuned_sentiment_model"
trainer.save_model(fine_tuned_model_path)

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ç”¨äºæ¨ç†
sentiment_pipeline = pipeline(
    "text-classification",
    model=fine_tuned_model_path,
    tokenizer=tokenizer,
    return_all_scores=True
)

# è¯»å– IMDB æ•°æ®é›†è¿›è¡Œæ¨ç†
dataset_path = "IMDB_Dataset.csv"
df = pd.read_csv(dataset_path)

# ä½¿ç”¨ fine-tuned æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ
df["predicted_sentiment"] = df["review"].apply(lambda x: sentiment_pipeline(x[:256])[0]['label'])

# å°†æ ‡ç­¾æ˜ å°„ä¸º 0ï¼ˆè´Ÿé¢ï¼‰æˆ– 1ï¼ˆæ­£é¢ï¼‰
df["predicted_sentiment"] = df["predicted_sentiment"].map({"LABEL_1": 1, "LABEL_0": 0})

# ä¿å­˜ç»“æœ
output_path = "IMDB_Dataset_labeled.csv"
df.to_csv(output_path, index=False)

print(f"Labeled dataset saved as '{output_path}'.")

# ğŸ¯ æ·»åŠ  Stretch Goal: è®©ç”¨æˆ·è¾“å…¥ä¸€ä¸ªå½±è¯„ï¼Œè¾“å‡ºæƒ…æ„Ÿåˆ†æç»“æœ
while True:
    user_input = input("è¾“å…¥ä¸€æ¡ç”µå½±è¯„è®º (è¾“å…¥ 'exit' é€€å‡º): ")
    if user_input.lower() == "exit":
        break
    prediction = sentiment_pipeline(user_input)
    print("æƒ…æ„Ÿåˆ†æç»“æœ:", prediction)
